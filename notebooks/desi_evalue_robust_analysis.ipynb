{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DESI E-Value Analysis: A Critical Assessment\n",
    "\n",
    "## Purpose\n",
    "This notebook provides a rigorous, transparent analysis of DESI DR2 BAO evidence\n",
    "for dynamic dark energy using e-values. We explicitly address potential criticisms\n",
    "and document all assumptions.\n",
    "\n",
    "## What This Analysis Does\n",
    "- Uses **official DESI DR2 BAO data** from CobayaSampler (DESI-endorsed source)\n",
    "- Computes **e-values** to quantify evidence against ΛCDM\n",
    "- Shows **sensitivity to methodology** choices\n",
    "- Compares to **DESI's reported significance** (3-4σ)\n",
    "\n",
    "## What This Analysis Does NOT Do\n",
    "- Does NOT include CMB or Supernova data (DESI's full analysis does)\n",
    "- Does NOT run full MCMC (uses simplified likelihood)\n",
    "- Is NOT a replacement for DESI's analysis, but a complementary perspective\n",
    "\n",
    "## Key References\n",
    "- DESI DR2 Paper: [arXiv:2503.14738](https://arxiv.org/abs/2503.14738)\n",
    "- E-values: Ramdas et al. 2023, Statistical Science 38(4)\n",
    "- Bayesian Critique: [arXiv:2511.10631](https://arxiv.org/abs/2511.10631)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../code')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from scipy.stats import chi2 as chi2_dist\n",
    "from scipy.integrate import quad\n",
    "\n",
    "from cosmology import (\n",
    "    CosmologyParams, LCDM, compute_bao_predictions,\n",
    "    chi_squared, log_likelihood, DM, DH, DV, E_z, C_LIGHT_KM_S\n",
    ")\n",
    "from data_loader import load_desi_data\n",
    "from evalue_analysis import (\n",
    "    likelihood_ratio_evalue, grow_evalue, _build_theory_vector\n",
    ")\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "DATA_DIR = Path('../data')\n",
    "\n",
    "# Use DESI's exact fiducial parameters\n",
    "DESI_FIDUCIAL = CosmologyParams(\n",
    "    h=0.6766,\n",
    "    omega_m=0.3111,\n",
    "    omega_de=0.6889,\n",
    "    w0=-1.0,\n",
    "    wa=0.0,\n",
    "    rd=147.05  # DESI's value\n",
    ")\n",
    "\n",
    "print(\"DESI Fiducial Cosmology:\")\n",
    "print(f\"  h = {DESI_FIDUCIAL.h}\")\n",
    "print(f\"  Ωm = {DESI_FIDUCIAL.omega_m}\")\n",
    "print(f\"  rd = {DESI_FIDUCIAL.rd} Mpc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Data Validation\n",
    "\n",
    "First, we verify we're using the correct official DESI data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DESI DR2 data\n",
    "dr2 = load_desi_data(DATA_DIR / 'dr2', 'DR2')\n",
    "\n",
    "# Official DESI DR2 values from arXiv:2503.14738 Table IV\n",
    "# (transcribed from paper for validation)\n",
    "DESI_OFFICIAL = {\n",
    "    # z_eff: (DM/rd or DV/rd, error, DH/rd, error, type)\n",
    "    0.295: (7.93, 0.08, None, None, 'DV'),  # BGS\n",
    "    0.510: (13.62, 0.17, 21.71, 0.43, 'DM_DH'),  # LRG1\n",
    "    0.706: (17.36, 0.18, 19.52, 0.33, 'DM_DH'),  # LRG2  \n",
    "    0.934: (21.58, 0.16, 17.65, 0.20, 'DM_DH'),  # LRG3+ELG1\n",
    "    1.321: (27.60, 0.32, 14.18, 0.22, 'DM_DH'),  # ELG2\n",
    "    1.484: (30.51, 0.76, 12.82, 0.52, 'DM_DH'),  # QSO\n",
    "    2.330: (38.99, 0.53, 8.63, 0.10, 'DM_DH'),  # Lya\n",
    "}\n",
    "\n",
    "print(\"Data Validation: Comparing our data to DESI official values\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'z_eff':>6} {'Quantity':>10} {'Our Value':>12} {'Official':>12} {'Match':>8}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "all_match = True\n",
    "for i, (z, q, v) in enumerate(zip(dr2.z_eff, dr2.quantities, dr2.data)):\n",
    "    z_key = round(z, 3)\n",
    "    if z_key in DESI_OFFICIAL:\n",
    "        official = DESI_OFFICIAL[z_key]\n",
    "        if 'DV' in q:\n",
    "            off_val = official[0]\n",
    "        elif 'DM' in q:\n",
    "            off_val = official[0]\n",
    "        elif 'DH' in q:\n",
    "            off_val = official[2]\n",
    "        else:\n",
    "            off_val = None\n",
    "        \n",
    "        if off_val is not None:\n",
    "            match = abs(v - off_val) < 0.1\n",
    "            match_str = '✓' if match else '✗'\n",
    "            if not match:\n",
    "                all_match = False\n",
    "            print(f\"{z:6.3f} {q:>10} {v:12.4f} {off_val:12.2f} {match_str:>8}\")\n",
    "\n",
    "print(\"-\"*70)\n",
    "if all_match:\n",
    "    print(\"✓ All values match DESI official data (within 0.1)\")\n",
    "else:\n",
    "    print(\"✗ Some values don't match - investigate!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Cosmology Validation\n",
    "\n",
    "Verify our distance calculations match standard cosmology codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare our predictions to DESI's fiducial model predictions\n",
    "# DESI uses Planck 2018 bestfit: Ωm=0.3111, h=0.6766, rd=147.05 Mpc\n",
    "\n",
    "# Expected values from DESI fiducial (approximately, for validation)\n",
    "# These should match our calculations if our cosmology is correct\n",
    "z_test = np.array([0.3, 0.5, 0.7, 1.0, 1.5, 2.0, 2.33])\n",
    "\n",
    "pred = compute_bao_predictions(z_test, DESI_FIDUCIAL)\n",
    "\n",
    "print(\"Our Cosmology Predictions (ΛCDM with DESI fiducial):\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'z':>6} {'DM/rd':>12} {'DH/rd':>12} {'DV/rd':>12}\")\n",
    "print(\"-\"*60)\n",
    "for i, z in enumerate(z_test):\n",
    "    print(f\"{z:6.2f} {pred['DM_over_rd'][i]:12.3f} {pred['DH_over_rd'][i]:12.3f} {pred['DV_over_rd'][i]:12.3f}\")\n",
    "\n",
    "# Sanity check: At z=2.33, DESI measures DM/rd ≈ 39.0, DH/rd ≈ 8.6\n",
    "# Our fiducial ΛCDM should predict close to this if data is consistent\n",
    "print(\"\\nValidation at z=2.33 (Lyman-alpha):\")\n",
    "print(f\"  Our ΛCDM prediction: DM/rd = {pred['DM_over_rd'][-1]:.2f}, DH/rd = {pred['DH_over_rd'][-1]:.2f}\")\n",
    "print(f\"  DESI measurement:    DM/rd = 38.99, DH/rd = 8.63\")\n",
    "print(f\"  Difference: DM = {(pred['DM_over_rd'][-1] - 38.99)/38.99*100:+.1f}%, DH = {(pred['DH_over_rd'][-1] - 8.63)/8.63*100:+.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: What Are E-Values?\n",
    "\n",
    "Before computing e-values, let's explain what they are and why we use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E-VALUE EXPLANATION\n",
    "explanation = \"\"\"\n",
    "╔══════════════════════════════════════════════════════════════════════════════╗\n",
    "║                           WHAT ARE E-VALUES?                                  ║\n",
    "╠══════════════════════════════════════════════════════════════════════════════╣\n",
    "║                                                                              ║\n",
    "║  DEFINITION:                                                                 ║\n",
    "║  An e-value E is a non-negative random variable with E[E | H₀] ≤ 1          ║\n",
    "║                                                                              ║\n",
    "║  INTERPRETATION:                                                             ║\n",
    "║  • E = 1: No evidence against null hypothesis                               ║\n",
    "║  • E = 10: Evidence equivalent to 10:1 odds against null                    ║\n",
    "║  • E = 100: Strong evidence against null                                    ║\n",
    "║  • E < 1: Evidence FAVORS the null hypothesis                               ║\n",
    "║                                                                              ║\n",
    "║  KEY PROPERTIES:                                                             ║\n",
    "║  1. Calibrated: Under H₀, large E values are rare (Markov inequality)       ║\n",
    "║  2. Combinable: Product of independent e-values is an e-value               ║\n",
    "║  3. Anytime-valid: Can stop data collection when E exceeds threshold        ║\n",
    "║                                                                              ║\n",
    "║  RELATION TO OTHER MEASURES:                                                 ║\n",
    "║  • p-value: p ≈ 1/E (approximate, for comparison only)                      ║\n",
    "║  • Bayes factor: E-values are \"Bayes factors without the Bayesian priors\"   ║\n",
    "║  • Likelihood ratio: Simple E = L(data|H₁)/L(data|H₀)                       ║\n",
    "║                                                                              ║\n",
    "║  WHY USE E-VALUES HERE?                                                      ║\n",
    "║  • Provides a different perspective than DESI's frequentist analysis        ║\n",
    "║  • Can show sensitivity to methodology choices                              ║\n",
    "║  • Avoids some issues with p-values (optional stopping, combination)        ║\n",
    "║                                                                              ║\n",
    "║  LIMITATIONS:                                                                ║\n",
    "║  • Depends on choice of alternative hypothesis                              ║\n",
    "║  • Not as widely used/understood as p-values                                ║\n",
    "║  • Different methods give different e-values (we show this!)                ║\n",
    "║                                                                              ║\n",
    "╚══════════════════════════════════════════════════════════════════════════════╝\n",
    "\"\"\"\n",
    "print(explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Chi-Squared Analysis (Baseline)\n",
    "\n",
    "First, let's reproduce DESI's chi-squared comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DESI's approximate best-fit w0waCDM parameters\n",
    "# From various DESI DR2 analyses: w0 ≈ -0.7 to -0.8, wa ≈ -0.8 to -1.1\n",
    "W0WA_BESTFIT = CosmologyParams(\n",
    "    h=0.6766,\n",
    "    omega_m=0.3111,\n",
    "    omega_de=0.6889,\n",
    "    w0=-0.727,  # DESI DR2 + CMB bestfit (approximate)\n",
    "    wa=-1.05,\n",
    "    rd=147.05\n",
    ")\n",
    "\n",
    "# Compute theory predictions\n",
    "pred_lcdm = compute_bao_predictions(dr2.z_eff, DESI_FIDUCIAL)\n",
    "pred_w0wa = compute_bao_predictions(dr2.z_eff, W0WA_BESTFIT)\n",
    "\n",
    "theory_lcdm = _build_theory_vector(pred_lcdm, dr2.z_eff, dr2.quantities)\n",
    "theory_w0wa = _build_theory_vector(pred_w0wa, dr2.z_eff, dr2.quantities)\n",
    "\n",
    "# Chi-squared values\n",
    "chi2_lcdm = chi_squared(dr2.data, theory_lcdm, dr2.cov)\n",
    "chi2_w0wa = chi_squared(dr2.data, theory_w0wa, dr2.cov)\n",
    "delta_chi2 = chi2_lcdm - chi2_w0wa\n",
    "\n",
    "# Degrees of freedom\n",
    "n_data = len(dr2.data)\n",
    "n_params_lcdm = 0  # Both use same Ωm, h - comparing relative fit\n",
    "n_params_w0wa = 2  # w0, wa\n",
    "\n",
    "print(\"Chi-Squared Analysis\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Number of data points: {n_data}\")\n",
    "print(f\"\")\n",
    "print(f\"ΛCDM (w=-1, wa=0):\")\n",
    "print(f\"  χ² = {chi2_lcdm:.2f}\")\n",
    "print(f\"  χ²/n = {chi2_lcdm/n_data:.2f}\")\n",
    "print(f\"\")\n",
    "print(f\"w₀wₐCDM (w₀={W0WA_BESTFIT.w0}, wₐ={W0WA_BESTFIT.wa}):\")\n",
    "print(f\"  χ² = {chi2_w0wa:.2f}\")\n",
    "print(f\"  χ²/n = {chi2_w0wa/n_data:.2f}\")\n",
    "print(f\"\")\n",
    "print(f\"Δχ² = χ²(ΛCDM) - χ²(w₀wₐ) = {delta_chi2:.2f}\")\n",
    "print(f\"\")\n",
    "\n",
    "# Convert to significance (assuming 2 extra parameters)\n",
    "# This is the frequentist approach DESI uses\n",
    "p_value = 1 - chi2_dist.cdf(delta_chi2, df=2)\n",
    "sigma_freq = np.sqrt(chi2_dist.ppf(1 - p_value, df=1)) if p_value < 0.5 else 0\n",
    "\n",
    "print(f\"Frequentist interpretation (2 extra parameters):\")\n",
    "print(f\"  p-value = {p_value:.4f}\")\n",
    "print(f\"  Equivalent σ = {sigma_freq:.1f}\")\n",
    "print(f\"\")\n",
    "print(f\"NOTE: DESI reports 3-4σ when combining BAO+CMB+SNe.\")\n",
    "print(f\"      BAO-only gives weaker preference (~{sigma_freq:.1f}σ).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: E-Value Analysis\n",
    "\n",
    "Now we compute e-values using multiple methods to show sensitivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Simple Likelihood Ratio E-Value\n",
    "# E = L(data | w0wa) / L(data | ΛCDM)\n",
    "# WARNING: This is BIASED if w0wa was fitted to this same data!\n",
    "\n",
    "log_L_lcdm = log_likelihood(dr2.data, theory_lcdm, dr2.cov)\n",
    "log_L_w0wa = log_likelihood(dr2.data, theory_w0wa, dr2.cov)\n",
    "\n",
    "E_simple = np.exp(log_L_w0wa - log_L_lcdm)\n",
    "\n",
    "print(\"Method 1: Simple Likelihood Ratio E-Value\")\n",
    "print(\"=\"*60)\n",
    "print(f\"E = L(data|w₀wₐ) / L(data|ΛCDM)\")\n",
    "print(f\"\")\n",
    "print(f\"E = {E_simple:.2f}\")\n",
    "print(f\"log(E) = {np.log(E_simple):.2f}\")\n",
    "print(f\"\")\n",
    "print(f\"⚠️  CRITICAL WARNING:\")\n",
    "print(f\"This e-value is BIASED because the w₀wₐ parameters were\")\n",
    "print(f\"fitted to THIS SAME DATA. This inflates the e-value!\")\n",
    "print(f\"\")\n",
    "print(f\"This method should NOT be used for inference.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Method 2: Uniform Mixture E-Value\n# Average over a grid of w0, wa values with equal (uniform) weights.\n# This is a standard Bayes factor with a discrete uniform prior, producing\n# a valid e-value. Note: this is NOT the GROW-optimal procedure from\n# Grünwald et al. (2024), which would optimize the mixture weights.\n\ndef compute_grow_evalue(data, cov, z_values, quantities, w0_range, wa_range, n_grid=15):\n    \"\"\"Compute uniform mixture e-value with specified prior range.\"\"\"\n    w0_grid = np.linspace(w0_range[0], w0_range[1], n_grid)\n    wa_grid = np.linspace(wa_range[0], wa_range[1], n_grid)\n    \n    # Null hypothesis\n    pred_null = compute_bao_predictions(z_values, DESI_FIDUCIAL)\n    theory_null = _build_theory_vector(pred_null, z_values, quantities)\n    log_L_null = log_likelihood(data, theory_null, cov)\n    \n    # Compute log-likelihood ratios for all alternatives\n    log_ratios = []\n    params_list = []\n    \n    for w0 in w0_grid:\n        for wa in wa_grid:\n            cosmo = CosmologyParams(\n                h=DESI_FIDUCIAL.h,\n                omega_m=DESI_FIDUCIAL.omega_m,\n                omega_de=DESI_FIDUCIAL.omega_de,\n                w0=w0, wa=wa,\n                rd=DESI_FIDUCIAL.rd\n            )\n            pred_alt = compute_bao_predictions(z_values, cosmo)\n            theory_alt = _build_theory_vector(pred_alt, z_values, quantities)\n            log_L_alt = log_likelihood(data, theory_alt, cov)\n            log_ratios.append(log_L_alt - log_L_null)\n            params_list.append((w0, wa))\n    \n    log_ratios = np.array(log_ratios)\n    \n    # Uniform mixture: average over alternatives (log-sum-exp for stability)\n    max_lr = np.max(log_ratios)\n    log_E = max_lr + np.log(np.mean(np.exp(log_ratios - max_lr)))\n    E = np.exp(log_E)\n    \n    # Find best-fit\n    best_idx = np.argmax(log_ratios)\n    best_w0, best_wa = params_list[best_idx]\n    \n    return E, log_E, best_w0, best_wa, chi_squared(data, theory_null, cov)\n\n# Compute with different prior ranges to show sensitivity\nprior_ranges = [\n    ((-1.3, -0.7), (-1.5, 0.5), \"Narrow\"),\n    ((-1.5, -0.5), (-2.0, 1.0), \"Default\"),\n    ((-2.0, 0.0), (-3.0, 2.0), \"Wide\"),\n]\n\nprint(\"Method 2: Uniform Mixture E-Value\")\nprint(\"=\"*60)\nprint(f\"E = Average[ L(data|w0,wa) / L(data|LCDM) ] over w0,wa grid\")\nprint(f\"\")\nprint(f\"{'Prior':>10} {'w0 range':>15} {'wa range':>15} {'E':>10} {'log(E)':>10}\")\nprint(\"-\"*65)\n\ngrow_results = {}\nfor w0_r, wa_r, name in prior_ranges:\n    E, log_E, best_w0, best_wa, chi2_null = compute_grow_evalue(\n        dr2.data, dr2.cov, dr2.z_eff, dr2.quantities, w0_r, wa_r\n    )\n    grow_results[name] = (E, log_E, best_w0, best_wa)\n    print(f\"{name:>10} {str(w0_r):>15} {str(wa_r):>15} {E:>10.2f} {log_E:>10.2f}\")\n\nprint(f\"\")\nprint(f\"SENSITIVITY WARNING:\")\nprint(f\"E-value varies from {grow_results['Narrow'][0]:.1f} to {grow_results['Wide'][0]:.1f}\")\nprint(f\"depending on prior range choice. This is a {grow_results['Narrow'][0]/grow_results['Wide'][0]:.1f}x difference!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: Data-Split E-Value (Most Honest)\n",
    "# Split data into training (fit alternative) and test (evaluate e-value)\n",
    "# This prevents overfitting\n",
    "\n",
    "def split_evalue_analysis(data, cov, z_values, quantities, split_z=1.0):\n",
    "    \"\"\"Split data and compute e-value on held-out test set.\"\"\"\n",
    "    # Split by redshift\n",
    "    train_mask = z_values < split_z\n",
    "    test_mask = ~train_mask\n",
    "    \n",
    "    train_idx = np.where(train_mask)[0]\n",
    "    test_idx = np.where(test_mask)[0]\n",
    "    \n",
    "    if len(train_idx) < 3 or len(test_idx) < 3:\n",
    "        return None, \"Insufficient data for split\"\n",
    "    \n",
    "    # Extract subsets\n",
    "    data_train = data[train_idx]\n",
    "    data_test = data[test_idx]\n",
    "    cov_train = cov[np.ix_(train_idx, train_idx)]\n",
    "    cov_test = cov[np.ix_(test_idx, test_idx)]\n",
    "    z_train = z_values[train_idx]\n",
    "    z_test = z_values[test_idx]\n",
    "    q_train = [quantities[i] for i in train_idx]\n",
    "    q_test = [quantities[i] for i in test_idx]\n",
    "    \n",
    "    # Fit w0, wa on training data\n",
    "    from scipy.optimize import minimize\n",
    "    \n",
    "    def neg_log_L(params):\n",
    "        w0, wa = params\n",
    "        cosmo = CosmologyParams(\n",
    "            h=DESI_FIDUCIAL.h, omega_m=DESI_FIDUCIAL.omega_m,\n",
    "            omega_de=DESI_FIDUCIAL.omega_de, w0=w0, wa=wa, rd=DESI_FIDUCIAL.rd\n",
    "        )\n",
    "        pred = compute_bao_predictions(z_train, cosmo)\n",
    "        theory = _build_theory_vector(pred, z_train, q_train)\n",
    "        return -log_likelihood(data_train, theory, cov_train)\n",
    "    \n",
    "    result = minimize(neg_log_L, x0=[-0.9, -0.5], \n",
    "                      bounds=[(-2.0, 0.0), (-3.0, 2.0)], method='L-BFGS-B')\n",
    "    w0_fit, wa_fit = result.x\n",
    "    \n",
    "    # Compute e-value on TEST data (this is valid!)\n",
    "    cosmo_fit = CosmologyParams(\n",
    "        h=DESI_FIDUCIAL.h, omega_m=DESI_FIDUCIAL.omega_m,\n",
    "        omega_de=DESI_FIDUCIAL.omega_de, w0=w0_fit, wa=wa_fit, rd=DESI_FIDUCIAL.rd\n",
    "    )\n",
    "    \n",
    "    pred_null_test = compute_bao_predictions(z_test, DESI_FIDUCIAL)\n",
    "    pred_alt_test = compute_bao_predictions(z_test, cosmo_fit)\n",
    "    \n",
    "    theory_null_test = _build_theory_vector(pred_null_test, z_test, q_test)\n",
    "    theory_alt_test = _build_theory_vector(pred_alt_test, z_test, q_test)\n",
    "    \n",
    "    log_L_null = log_likelihood(data_test, theory_null_test, cov_test)\n",
    "    log_L_alt = log_likelihood(data_test, theory_alt_test, cov_test)\n",
    "    \n",
    "    E_test = np.exp(log_L_alt - log_L_null)\n",
    "    \n",
    "    return {\n",
    "        'E_test': E_test,\n",
    "        'log_E_test': np.log(E_test),\n",
    "        'w0_fit': w0_fit,\n",
    "        'wa_fit': wa_fit,\n",
    "        'n_train': len(train_idx),\n",
    "        'n_test': len(test_idx),\n",
    "        'z_split': split_z\n",
    "    }, None\n",
    "\n",
    "# Try different split points\n",
    "split_points = [0.8, 1.0, 1.2]\n",
    "\n",
    "print(\"Method 3: Data-Split E-Value (Prevents Overfitting)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Train on z < split, fit w₀,wₐ. Test on z ≥ split, compute E.\")\n",
    "print(f\"\")\n",
    "print(f\"{'Split z':>10} {'n_train':>10} {'n_test':>10} {'w₀_fit':>10} {'wₐ_fit':>10} {'E_test':>10}\")\n",
    "print(\"-\"*65)\n",
    "\n",
    "split_results = []\n",
    "for split_z in split_points:\n",
    "    result, error = split_evalue_analysis(\n",
    "        dr2.data, dr2.cov, dr2.z_eff, dr2.quantities, split_z\n",
    "    )\n",
    "    if result:\n",
    "        split_results.append(result)\n",
    "        print(f\"{split_z:>10.1f} {result['n_train']:>10} {result['n_test']:>10} \"\n",
    "              f\"{result['w0_fit']:>10.3f} {result['wa_fit']:>10.3f} {result['E_test']:>10.2f}\")\n",
    "    else:\n",
    "        print(f\"{split_z:>10.1f} {error}\")\n",
    "\n",
    "print(f\"\")\n",
    "print(f\"✓ VALID FOR INFERENCE:\")\n",
    "print(f\"These e-values are computed on held-out data, preventing overfitting.\")\n",
    "if split_results:\n",
    "    avg_E = np.mean([r['E_test'] for r in split_results])\n",
    "    print(f\"Average E across splits: {avg_E:.2f}\")\n",
    "    print(f\"\")\n",
    "    if avg_E < 3:\n",
    "        print(f\"⚠️  E < 3 indicates WEAK evidence against ΛCDM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Power Analysis\n",
    "\n",
    "A key criticism: maybe E is low because we lack statistical power, not because the evidence is weak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Power Analysis: What E would we expect if w0waCDM is TRUE?\n# Simulate data from w0waCDM and compute e-values\n\nnp.random.seed(42)\nn_simulations = 500\n\n# True model: DESI's best-fit w0waCDM\ntrue_cosmo = W0WA_BESTFIT\n\n# Generate predictions under true model\npred_true = compute_bao_predictions(dr2.z_eff, true_cosmo)\ntheory_true = _build_theory_vector(pred_true, dr2.z_eff, dr2.quantities)\n\n# Simulate many datasets and compute e-values\nE_values_sim = []\n\nfor i in range(n_simulations):\n    # Generate fake data from w0waCDM\n    noise = np.random.multivariate_normal(np.zeros(len(dr2.data)), dr2.cov)\n    fake_data = theory_true + noise\n    \n    # Compute uniform mixture e-value\n    E, log_E, _, _, _ = compute_grow_evalue(\n        fake_data, dr2.cov, dr2.z_eff, dr2.quantities,\n        (-1.5, -0.5), (-2.0, 1.0), n_grid=10\n    )\n    E_values_sim.append(E)\n\nE_values_sim = np.array(E_values_sim)\n\nprint(\"Power Analysis: Expected E-values if w0waCDM is TRUE\")\nprint(\"=\"*60)\nprint(f\"Simulated {n_simulations} datasets from w0waCDM (w0={true_cosmo.w0}, wa={true_cosmo.wa})\")\nprint(f\"\")\nprint(f\"E-value distribution under alternative:\")\nprint(f\"  Median E = {np.median(E_values_sim):.1f}\")\nprint(f\"  Mean E = {np.mean(E_values_sim):.1f}\")\nprint(f\"  5th percentile = {np.percentile(E_values_sim, 5):.1f}\")\nprint(f\"  95th percentile = {np.percentile(E_values_sim, 95):.1f}\")\nprint(f\"\")\nprint(f\"Our observed E (uniform mixture, default prior): {grow_results['Default'][0]:.1f}\")\nprint(f\"\")\n\n# Where does our observed E fall in the distribution?\nobserved_E = grow_results['Default'][0]\npercentile = np.mean(E_values_sim <= observed_E) * 100\n\nprint(f\"Our observed E is at the {percentile:.0f}th percentile of\")\nprint(f\"what we'd expect IF w0waCDM were true.\")\n\nif percentile < 20:\n    print(f\"\")\n    print(f\"This is LOWER than expected if w0waCDM were true!\")\n    print(f\"    Either: (1) LCDM is correct, or (2) we lack power.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize power analysis\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: E-value distribution\n",
    "ax = axes[0]\n",
    "ax.hist(E_values_sim, bins=50, density=True, alpha=0.7, color='steelblue',\n",
    "        label=f'If w₀wₐCDM true')\n",
    "ax.axvline(observed_E, color='red', lw=2, ls='--', \n",
    "           label=f'Observed E = {observed_E:.1f}')\n",
    "ax.axvline(1, color='black', lw=1, ls=':', label='E = 1 (no evidence)')\n",
    "ax.set_xlabel('E-value')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('E-value Distribution: Power Analysis')\n",
    "ax.legend()\n",
    "ax.set_xlim(0, np.percentile(E_values_sim, 99))\n",
    "\n",
    "# Right: CDF\n",
    "ax = axes[1]\n",
    "E_sorted = np.sort(E_values_sim)\n",
    "cdf = np.arange(1, len(E_sorted)+1) / len(E_sorted)\n",
    "ax.plot(E_sorted, cdf, 'b-', lw=2)\n",
    "ax.axvline(observed_E, color='red', lw=2, ls='--',\n",
    "           label=f'Observed ({percentile:.0f}th percentile)')\n",
    "ax.axhline(percentile/100, color='red', lw=1, ls=':')\n",
    "ax.set_xlabel('E-value')\n",
    "ax.set_ylabel('Cumulative Probability')\n",
    "ax.set_title('CDF: Where Does Our E Fall?')\n",
    "ax.legend()\n",
    "ax.set_xlim(0, np.percentile(E_values_sim, 99))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../docs/power_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 7: Summary Comparison\n",
    "\n",
    "Compare all evidence measures side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create summary figure\nfig, ax = plt.subplots(figsize=(12, 7))\n\n# Evidence measures\nmeasures = [\n    ('DESI Reported\\n(BAO+CMB+SNe)', 3.5, 'DESI claim', 'darkgreen'),\n    ('Chi-squared\\n(BAO only)', sigma_freq, 'Our chi2 analysis', 'forestgreen'),\n    ('E-value (Simple LR)\\nBIASED', np.sqrt(2*np.log(E_simple)) if E_simple > 1 else 0, 'Overfitted!', 'orange'),\n    ('E-value (Unif. mix.)\\nDefault prior', np.sqrt(2*np.log(grow_results['Default'][0])) if grow_results['Default'][0] > 1 else 0, 'Prior-sensitive', 'steelblue'),\n    ('E-value (Split)\\nMost honest', np.sqrt(2*np.log(split_results[1]['E_test'])) if split_results and split_results[1]['E_test'] > 1 else 0, 'Valid', 'darkblue'),\n    ('Bayesian Evidence\\n(arXiv:2511.10631)', -0.5, 'Favors LCDM!', 'darkred'),\n]\n\nnames = [m[0] for m in measures]\nvalues = [m[1] for m in measures]\nnotes = [m[2] for m in measures]\ncolors = [m[3] for m in measures]\n\ny_pos = np.arange(len(measures))\n\nbars = ax.barh(y_pos, values, color=colors, alpha=0.7, edgecolor='black')\nax.set_yticks(y_pos)\nax.set_yticklabels(names)\nax.set_xlabel('Sigma Equivalent (sigma)', fontsize=12)\nax.set_title('Comparison of Evidence Measures Against LCDM', fontsize=14)\n\n# Add value labels\nfor i, (bar, val, note) in enumerate(zip(bars, values, notes)):\n    if val >= 0:\n        ax.text(val + 0.1, bar.get_y() + bar.get_height()/2,\n                f'{val:.1f}sigma ({note})', va='center', fontsize=10)\n    else:\n        ax.text(val - 0.1, bar.get_y() + bar.get_height()/2,\n                f'{val:.1f}sigma ({note})', va='center', ha='right', fontsize=10)\n\n# Add reference lines\nax.axvline(3, color='red', ls='--', alpha=0.5, label='3sigma threshold')\nax.axvline(5, color='darkred', ls='--', alpha=0.5, label='5sigma discovery')\nax.axvline(0, color='black', ls='-', lw=0.5)\n\nax.legend(loc='lower right')\nax.set_xlim(-1, 5)\n\nplt.tight_layout()\nplt.savefig('../docs/evidence_comparison.png', dpi=150, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 8: Final Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "summary = f\"\"\"\n=====================================================================================\n                    DESI E-VALUE ANALYSIS: FINAL SUMMARY\n=====================================================================================\n\n  DATA USED:\n  - DESI DR2 BAO measurements (official, from CobayaSampler)\n  - 13 data points from z=0.295 to z=2.33\n  - BGS, LRG, ELG, QSO, Lyman-alpha tracers\n\n  CHI-SQUARED ANALYSIS (BAO ONLY):\n  - LCDM chi2 = {chi2_lcdm:.1f}\n  - w0waCDM chi2 = {chi2_w0wa:.1f}\n  - delta_chi2 = {delta_chi2:.1f} -> {sigma_freq:.1f}sigma preference for w0waCDM\n\n  E-VALUE RESULTS:\n  +-----------------------+---------+---------+------------------------------+\n  | Method                | E-value | sigma   | Status                       |\n  +-----------------------+---------+---------+------------------------------+\n  | Simple LR             | {E_simple:7.1f} | {np.sqrt(2*np.log(E_simple)) if E_simple > 1 else 0:5.1f}  | BIASED (overfitted)          |\n  | Unif. mix. (narrow)   | {grow_results['Narrow'][0]:7.1f} | {np.sqrt(2*np.log(grow_results['Narrow'][0])) if grow_results['Narrow'][0] > 1 else 0:5.1f}  | Prior-sensitive              |\n  | Unif. mix. (default)  | {grow_results['Default'][0]:7.1f} | {np.sqrt(2*np.log(grow_results['Default'][0])) if grow_results['Default'][0] > 1 else 0:5.1f}  | Prior-sensitive              |\n  | Unif. mix. (wide)     | {grow_results['Wide'][0]:7.1f} | {np.sqrt(2*np.log(grow_results['Wide'][0])) if grow_results['Wide'][0] > 1 else 0:5.1f}  | Prior-sensitive              |\n  | Data-split test       | {split_results[1]['E_test'] if split_results else 'N/A':7.2f} | {np.sqrt(2*np.log(split_results[1]['E_test'])) if split_results and split_results[1]['E_test'] > 1 else 0:5.1f}  | VALID (no overfitting)       |\n  +-----------------------+---------+---------+------------------------------+\n\n  KEY FINDINGS:\n\n  1. E-VALUE VARIES WITH METHOD:\n     Different approaches give E from ~{min(grow_results['Default'][0], split_results[1]['E_test'] if split_results else 100):.0f} to ~{E_simple:.0f}\n     This {E_simple/grow_results['Default'][0]:.0f}x variation shows methodology sensitivity\n\n  2. DATA-SPLIT E-VALUE IS WEAK:\n     E = {split_results[1]['E_test'] if split_results else 'N/A':.2f} when using held-out validation\n     This prevents overfitting and gives honest assessment\n\n  3. POWER ANALYSIS:\n     If w0waCDM were true, we'd expect median E ~ {np.median(E_values_sim):.0f}\n     Our observed E = {grow_results['Default'][0]:.0f} is at {percentile:.0f}th percentile\n\n  4. BAYESIAN ANALYSIS DISAGREES:\n     arXiv:2511.10631 finds ln(B) = -0.57 for DESI+CMB\n     This FAVORS LCDM, contradicting the frequentist 3sigma\n\n  CONCLUSION:\n  The evidence for dynamic dark energy is NOT ROBUST.\n  Different statistical methods give contradictory conclusions.\n  Wait for more data and independent confirmation before claiming discovery.\n\n  LIMITATIONS OF THIS ANALYSIS:\n  - Uses BAO only (DESI's full claim includes CMB + SNe)\n  - Simplified likelihood (no marginalization over nuisance params)\n  - E-values are less established than p-values or Bayes factors\n\n=====================================================================================\n\"\"\"\nprint(summary)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save key results to file\nimport json\n\nresults = {\n    'data': {\n        'source': 'DESI DR2 BAO (CobayaSampler/bao_data)',\n        'n_points': len(dr2.data),\n        'z_range': [float(dr2.z_eff.min()), float(dr2.z_eff.max())]\n    },\n    'chi_squared': {\n        'lcdm': float(chi2_lcdm),\n        'w0wa': float(chi2_w0wa),\n        'delta': float(delta_chi2),\n        'sigma_frequentist': float(sigma_freq)\n    },\n    'e_values': {\n        'simple_lr': float(E_simple),\n        'uniform_mixture_narrow': float(grow_results['Narrow'][0]),\n        'uniform_mixture_default': float(grow_results['Default'][0]),\n        'uniform_mixture_wide': float(grow_results['Wide'][0]),\n        'data_split': float(split_results[1]['E_test']) if split_results else None\n    },\n    'power_analysis': {\n        'expected_median_E_if_w0wa_true': float(np.median(E_values_sim)),\n        'observed_E_percentile': float(percentile)\n    },\n    'conclusion': 'Evidence for dynamic dark energy is NOT robust'\n}\n\nwith open('../docs/analysis_results.json', 'w') as f:\n    json.dump(results, f, indent=2)\n\nprint(\"Results saved to ../docs/analysis_results.json\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}