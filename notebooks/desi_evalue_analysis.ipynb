{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DESI E-Value Analysis: Testing Dark Energy Evidence\n",
    "\n",
    "This notebook analyzes the DESI DR2 BAO measurements using e-values to compare\n",
    "LCDM against dynamic dark energy (w0waCDM) models.\n",
    "\n",
    "## Background\n",
    "\n",
    "The DESI collaboration reported 2.8-4.2 sigma evidence for dynamic dark energy\n",
    "in their DR2 results (March 2025). However, this evidence has been criticized:\n",
    "\n",
    "1. **Bayesian analysis favors LCDM** - ln B = -0.57 for DESI+CMB (arxiv:2511.10631)\n",
    "2. **Dataset tensions** - CMB, BAO, SNe show internal inconsistencies\n",
    "3. **w0waCDM may just resolve tensions** rather than detect new physics\n",
    "\n",
    "We use e-values to provide another perspective on this evidence.\n",
    "\n",
    "## References\n",
    "- DESI DR2: [arXiv:2503.14738](https://arxiv.org/abs/2503.14738)\n",
    "- E-values: Ramdas et al. (2023), Statistical Science 38(4)\n",
    "- Bayesian critique: [arXiv:2511.10631](https://arxiv.org/abs/2511.10631)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../code')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from cosmology import (\n",
    "    CosmologyParams, LCDM, DESI_DR2_BEST_FIT,\n",
    "    compute_bao_predictions, chi_squared, log_likelihood,\n",
    "    DM, DH, DV\n",
    ")\n",
    "from data_loader import load_desi_data, get_theory_vector, BAODataset\n",
    "from evalue_analysis import (\n",
    "    likelihood_ratio_evalue, split_evalue, grow_evalue,\n",
    "    summarize_evidence_caveats, _build_theory_vector\n",
    ")\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "DATA_DIR = Path('../data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load DESI Data\n",
    "\n",
    "We load both DR1 and DR2 BAO measurements from the official Cobaya data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "dr1 = load_desi_data(DATA_DIR / 'dr1', 'DR1')\n",
    "dr2 = load_desi_data(DATA_DIR / 'dr2', 'DR2')\n",
    "\n",
    "print(\"DESI DR1 Data:\")\n",
    "print(dr1.summary())\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "print(\"DESI DR2 Data:\")\n",
    "print(dr2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualize BAO Measurements\n",
    "\n",
    "Compare DESI measurements against LCDM and w0waCDM predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate theory predictions\n",
    "z_theory = np.linspace(0.1, 2.5, 100)\n",
    "\n",
    "pred_lcdm = compute_bao_predictions(z_theory, LCDM)\n",
    "pred_w0wa = compute_bao_predictions(z_theory, DESI_DR2_BEST_FIT)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# DM/rd plot\n",
    "ax = axes[0]\n",
    "ax.plot(z_theory, pred_lcdm['DM_over_rd'], 'b-', lw=2, label='LCDM')\n",
    "ax.plot(z_theory, pred_w0wa['DM_over_rd'], 'r--', lw=2, label='w0waCDM (DESI best-fit)')\n",
    "\n",
    "# DR2 data\n",
    "dm_mask = ['DM' in q for q in dr2.quantities]\n",
    "ax.errorbar(\n",
    "    dr2.z_eff[dm_mask], \n",
    "    dr2.data[dm_mask], \n",
    "    yerr=dr2.errors[dm_mask],\n",
    "    fmt='ko', capsize=3, markersize=8, label='DESI DR2'\n",
    ")\n",
    "\n",
    "ax.set_xlabel('Redshift z')\n",
    "ax.set_ylabel(r'$D_M / r_d$')\n",
    "ax.set_title('Transverse Comoving Distance')\n",
    "ax.legend()\n",
    "\n",
    "# DH/rd plot\n",
    "ax = axes[1]\n",
    "ax.plot(z_theory, pred_lcdm['DH_over_rd'], 'b-', lw=2, label='LCDM')\n",
    "ax.plot(z_theory, pred_w0wa['DH_over_rd'], 'r--', lw=2, label='w0waCDM')\n",
    "\n",
    "dh_mask = ['DH' in q for q in dr2.quantities]\n",
    "ax.errorbar(\n",
    "    dr2.z_eff[dh_mask], \n",
    "    dr2.data[dh_mask], \n",
    "    yerr=dr2.errors[dh_mask],\n",
    "    fmt='ko', capsize=3, markersize=8, label='DESI DR2'\n",
    ")\n",
    "\n",
    "ax.set_xlabel('Redshift z')\n",
    "ax.set_ylabel(r'$D_H / r_d$')\n",
    "ax.set_title('Hubble Distance')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../docs/bao_measurements.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chi-Squared Analysis\n",
    "\n",
    "First, let's do a traditional chi-squared comparison between models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute theory predictions at data redshifts\n",
    "pred_lcdm_data = compute_bao_predictions(dr2.z_eff, LCDM)\n",
    "pred_w0wa_data = compute_bao_predictions(dr2.z_eff, DESI_DR2_BEST_FIT)\n",
    "\n",
    "theory_lcdm = _build_theory_vector(pred_lcdm_data, dr2.z_eff, dr2.quantities)\n",
    "theory_w0wa = _build_theory_vector(pred_w0wa_data, dr2.z_eff, dr2.quantities)\n",
    "\n",
    "chi2_lcdm = chi_squared(dr2.data, theory_lcdm, dr2.cov)\n",
    "chi2_w0wa = chi_squared(dr2.data, theory_w0wa, dr2.cov)\n",
    "\n",
    "n_data = len(dr2.data)\n",
    "dof_lcdm = n_data - 0  # LCDM has no free parameters in this comparison\n",
    "dof_w0wa = n_data - 2  # w0waCDM has 2 extra parameters\n",
    "\n",
    "print(\"Chi-Squared Analysis (DESI DR2)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"N data points: {n_data}\")\n",
    "print(f\"\\nLCDM:\")\n",
    "print(f\"  chi2 = {chi2_lcdm:.2f}\")\n",
    "print(f\"  chi2/dof = {chi2_lcdm/dof_lcdm:.2f}\")\n",
    "print(f\"\\nw0waCDM (DESI best-fit w0={DESI_DR2_BEST_FIT.w0}, wa={DESI_DR2_BEST_FIT.wa}):\")\n",
    "print(f\"  chi2 = {chi2_w0wa:.2f}\")\n",
    "print(f\"  chi2/dof = {chi2_w0wa/dof_w0wa:.2f}\")\n",
    "print(f\"\\nDelta chi2 = {chi2_lcdm - chi2_w0wa:.2f}\")\n",
    "print(f\"Equivalent sigma (2 dof): {np.sqrt(chi2_lcdm - chi2_w0wa):.1f} sigma\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. E-Value Analysis\n",
    "\n",
    "Now compute e-values using different methods.\n",
    "\n",
    "### 4.1 Simple Likelihood Ratio E-Value\n",
    "\n",
    "The simplest e-value is just the likelihood ratio. This can overfit to data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_simple = likelihood_ratio_evalue(\n",
    "    dr2.data, dr2.cov, theory_lcdm, theory_w0wa\n",
    ")\n",
    "\n",
    "print(\"Simple Likelihood Ratio E-Value\")\n",
    "print(\"=\"*50)\n",
    "print(f\"E = {e_simple.e_value:.2f}\")\n",
    "print(f\"log(E) = {e_simple.log_e:.2f}\")\n",
    "print(f\"Sigma equivalent: {e_simple.sigma_equivalent:.1f}\")\n",
    "print(f\"\\nWARNING: This e-value uses the DESI best-fit as alternative,\")\n",
    "print(f\"which was fitted to THIS data. This inflates the e-value!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 GROW Mixture E-Value\n",
    "\n",
    "A more principled approach using a mixture over plausible alternatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_grow = grow_evalue(\n",
    "    dr2.data, dr2.cov, dr2.z_eff, dr2.quantities\n",
    ")\n",
    "\n",
    "print(\"GROW Mixture E-Value\")\n",
    "print(\"=\"*50)\n",
    "print(f\"E = {e_grow.e_value:.2f}\")\n",
    "print(f\"log(E) = {e_grow.log_e:.2f}\")\n",
    "print(f\"Sigma equivalent: {e_grow.sigma_equivalent:.1f}\")\n",
    "print(f\"\\nBest-fit alternative:\")\n",
    "print(f\"  w0 = {e_grow.alt_params.w0:.3f}\")\n",
    "print(f\"  wa = {e_grow.alt_params.wa:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Data-Split E-Value\n",
    "\n",
    "Split data into training (fit alternative) and test (evaluate e-value) sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    e_train, e_test = split_evalue(\n",
    "        dr2.data, dr2.cov, dr2.z_eff, dr2.quantities,\n",
    "        split_z=1.0  # Use z<1 to train, z>=1 to test\n",
    "    )\n",
    "    \n",
    "    print(\"Data-Split E-Value Analysis\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Split at z = 1.0\")\n",
    "    print(f\"\\nTraining set (z < 1.0):\")\n",
    "    print(f\"  E = {e_train.e_value:.2f} (NOT for inference - overfitted)\")\n",
    "    print(f\"  Fitted: w0={e_train.alt_params.w0:.3f}, wa={e_train.alt_params.wa:.3f}\")\n",
    "    print(f\"\\nTest set (z >= 1.0) - VALID FOR INFERENCE:\")\n",
    "    print(f\"  E = {e_test.e_value:.2f}\")\n",
    "    print(f\"  log(E) = {e_test.log_e:.2f}\")\n",
    "    print(f\"  Sigma equivalent: {e_test.sigma_equivalent:.1f}\")\n",
    "except ValueError as e:\n",
    "    print(f\"Split analysis failed: {e}\")\n",
    "    print(\"Need more data points in each split.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. DR1 vs DR2 Comparison\n",
    "\n",
    "Compare evidence from DR1 to DR2 to see how it evolved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DR1 analysis\n",
    "e_grow_dr1 = grow_evalue(\n",
    "    dr1.data, dr1.cov, dr1.z_eff, dr1.quantities\n",
    ")\n",
    "\n",
    "print(\"Comparison: DR1 vs DR2\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nDR1 ({len(dr1.data)} measurements):\")\n",
    "print(f\"  E = {e_grow_dr1.e_value:.2f}\")\n",
    "print(f\"  Sigma equivalent: {e_grow_dr1.sigma_equivalent:.1f}\")\n",
    "print(f\"  Best-fit: w0={e_grow_dr1.alt_params.w0:.3f}, wa={e_grow_dr1.alt_params.wa:.3f}\")\n",
    "\n",
    "print(f\"\\nDR2 ({len(dr2.data)} measurements):\")\n",
    "print(f\"  E = {e_grow.e_value:.2f}\")\n",
    "print(f\"  Sigma equivalent: {e_grow.sigma_equivalent:.1f}\")\n",
    "print(f\"  Best-fit: w0={e_grow.alt_params.w0:.3f}, wa={e_grow.alt_params.wa:.3f}\")\n",
    "\n",
    "print(f\"\\nRatio E_DR2 / E_DR1 = {e_grow.e_value / e_grow_dr1.e_value:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Critical Assessment\n",
    "\n",
    "Now let's discuss why e-values may be misleading here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summarize_evidence_caveats(e_grow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sensitivity Analysis\n",
    "\n",
    "How sensitive are the e-values to the choice of alternative hypothesis range?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sensitivity to prior range\n",
    "w0_ranges = [\n",
    "    (-1.2, -0.8),  # Narrow around LCDM\n",
    "    (-1.5, -0.5),  # Moderate (our default)\n",
    "    (-2.0, 0.0),   # Wide\n",
    "]\n",
    "\n",
    "wa_ranges = [\n",
    "    (-1.0, 0.5),   # Narrow\n",
    "    (-2.0, 1.0),   # Moderate (our default)\n",
    "    (-3.0, 2.0),   # Wide\n",
    "]\n",
    "\n",
    "print(\"Sensitivity to Prior Range\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'w0 range':>15} {'wa range':>15} {'E-value':>12} {'log(E)':>10}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for w0_r, wa_r in zip(w0_ranges, wa_ranges):\n",
    "    # Recompute with different grid\n",
    "    w0_grid = np.linspace(w0_r[0], w0_r[1], 10)\n",
    "    wa_grid = np.linspace(wa_r[0], wa_r[1], 10)\n",
    "    \n",
    "    # Compute likelihood ratios\n",
    "    log_ratios = []\n",
    "    pred_null = compute_bao_predictions(dr2.z_eff, LCDM)\n",
    "    theory_null = _build_theory_vector(pred_null, dr2.z_eff, dr2.quantities)\n",
    "    log_L_null = log_likelihood(dr2.data, theory_null, dr2.cov)\n",
    "    \n",
    "    for w0 in w0_grid:\n",
    "        for wa in wa_grid:\n",
    "            cosmo = CosmologyParams(w0=w0, wa=wa)\n",
    "            pred = compute_bao_predictions(dr2.z_eff, cosmo)\n",
    "            theory_alt = _build_theory_vector(pred, dr2.z_eff, dr2.quantities)\n",
    "            log_L_alt = log_likelihood(dr2.data, theory_alt, dr2.cov)\n",
    "            log_ratios.append(log_L_alt - log_L_null)\n",
    "    \n",
    "    log_ratios = np.array(log_ratios)\n",
    "    max_lr = np.max(log_ratios)\n",
    "    log_e = max_lr + np.log(np.mean(np.exp(log_ratios - max_lr)))\n",
    "    e_val = np.exp(log_e)\n",
    "    \n",
    "    print(f\"{str(w0_r):>15} {str(wa_r):>15} {e_val:>12.2f} {log_e:>10.2f}\")\n",
    "\n",
    "print(\"\\nNote: E-value varies significantly with prior choice!\")\n",
    "print(\"This is a fundamental limitation of the approach.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualization of Evidence Trajectory\n",
    "\n",
    "How does the e-value accumulate as we add data points?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential evidence accumulation\n",
    "# Sort by redshift and accumulate\n",
    "sort_idx = np.argsort(dr2.z_eff)\n",
    "\n",
    "cumulative_log_e = []\n",
    "z_cumulative = []\n",
    "\n",
    "for n in range(2, len(dr2.data) + 1):\n",
    "    idx = sort_idx[:n]\n",
    "    data_sub = dr2.data[idx]\n",
    "    cov_sub = dr2.cov[np.ix_(idx, idx)]\n",
    "    z_sub = dr2.z_eff[idx]\n",
    "    q_sub = [dr2.quantities[i] for i in idx]\n",
    "    \n",
    "    # Compute GROW e-value on subset\n",
    "    w0_grid = np.linspace(-1.5, -0.5, 8)\n",
    "    wa_grid = np.linspace(-2.0, 1.0, 8)\n",
    "    \n",
    "    log_ratios = []\n",
    "    pred_null = compute_bao_predictions(z_sub, LCDM)\n",
    "    theory_null = _build_theory_vector(pred_null, z_sub, q_sub)\n",
    "    log_L_null = log_likelihood(data_sub, theory_null, cov_sub)\n",
    "    \n",
    "    for w0 in w0_grid:\n",
    "        for wa in wa_grid:\n",
    "            cosmo = CosmologyParams(w0=w0, wa=wa)\n",
    "            pred = compute_bao_predictions(z_sub, cosmo)\n",
    "            theory_alt = _build_theory_vector(pred, z_sub, q_sub)\n",
    "            log_L_alt = log_likelihood(data_sub, theory_alt, cov_sub)\n",
    "            log_ratios.append(log_L_alt - log_L_null)\n",
    "    \n",
    "    log_ratios = np.array(log_ratios)\n",
    "    max_lr = np.max(log_ratios)\n",
    "    log_e = max_lr + np.log(np.mean(np.exp(log_ratios - max_lr)))\n",
    "    \n",
    "    cumulative_log_e.append(log_e)\n",
    "    z_cumulative.append(z_sub[-1])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.plot(z_cumulative, cumulative_log_e, 'b-o', lw=2, markersize=6)\n",
    "ax.axhline(0, color='k', linestyle='--', alpha=0.5, label='No evidence (E=1)')\n",
    "ax.axhline(np.log(10), color='orange', linestyle=':', alpha=0.7, label='E=10')\n",
    "ax.axhline(np.log(100), color='red', linestyle=':', alpha=0.7, label='E=100')\n",
    "\n",
    "ax.set_xlabel('Maximum redshift included')\n",
    "ax.set_ylabel('log(E-value)')\n",
    "ax.set_title('Cumulative Evidence vs Redshift')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../docs/evidence_trajectory.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusions\n",
    "\n",
    "### Summary of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"DESI E-VALUE ANALYSIS: SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n1. DESI DR2 BAO Measurements:\")\n",
    "print(f\"   - {len(dr2.data)} data points from z=0.295 to z=2.33\")\n",
    "print(f\"   - Tracers: BGS, LRG, ELG, QSO, Lyman-alpha\")\n",
    "\n",
    "print(f\"\\n2. Chi-Squared Results:\")\n",
    "print(f\"   - LCDM chi2 = {chi2_lcdm:.1f}\")\n",
    "print(f\"   - w0waCDM chi2 = {chi2_w0wa:.1f}\")\n",
    "print(f\"   - Delta chi2 = {chi2_lcdm - chi2_w0wa:.1f}\")\n",
    "\n",
    "print(f\"\\n3. E-Value Results:\")\n",
    "print(f\"   - Simple LR: E = {e_simple.e_value:.1f} (BIASED - overfitted)\")\n",
    "print(f\"   - GROW mixture: E = {e_grow.e_value:.1f} (~{e_grow.sigma_equivalent:.1f} sigma)\")\n",
    "\n",
    "print(f\"\\n4. KEY CAVEATS:\")\n",
    "print(f\"   a) Bayesian analysis FAVORS LCDM (ln B = -0.57 for DESI+CMB)\")\n",
    "print(f\"   b) E-values sensitive to prior range (varied 2-3x in our tests)\")\n",
    "print(f\"   c) Dataset tensions may drive apparent signal\")\n",
    "print(f\"   d) w0waCDM may be resolving tensions, not detecting physics\")\n",
    "\n",
    "print(f\"\\n5. RECOMMENDATION:\")\n",
    "print(f\"   The evidence for dynamic dark energy is NOT robust.\")\n",
    "print(f\"   The disagreement between frequentist (3-4 sigma) and Bayesian\")\n",
    "print(f\"   (favors LCDM) analyses suggests systematic issues.\")\n",
    "print(f\"   \")\n",
    "print(f\"   Wait for:\")\n",
    "print(f\"   - DESI DR3+ with more data\")\n",
    "print(f\"   - Resolution of dataset tensions\")\n",
    "print(f\"   - Independent confirmation (Euclid, Roman)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
